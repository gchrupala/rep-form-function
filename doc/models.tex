\section{Models}

In our analyses of the acquired linguist knowledge, we apply our methods to the following models:
\begin{itemize}
\item {\sc Imaginet}: A multi-modal Gated Recurrent Unit (GRU) network consisting of two
  pathways, {\sc Visual} and {\sc Textual}, coupled via word embeddings.
\item {\sc LM}: A (unimodal) language model consisting of a GRU network.
\item {\sc Sum}: A network with the same objective as the {\sc Visual}
  pathway of {\sc Imaginet}, but which uses sum of word embeddings
  instead of a GRU.
\end{itemize}
The rest of this section gives a detailed description of these models.

\subsection{Gated Recurrent Neural Networks}
\label{sec:gru}

One of the main difficulties for training traditional Elman networks
arises from the fact that they overwrite their hidden states at every
time step with a new value computed from the current input $\mathbf{x_{t}}$ and
the previous hidden state $\mathbf{h_{t-1}}$. Similarly to LSTMs,
Gated Recurrent Unit networks introduce a mechanism which facilitates the retention of 
information over multiple time steps.
Specifically, the GRU computes the hidden state at current time step $\mathbf{h}_{t}$, as the
linear combination of previous activation $\mathbf{h_{t-1}}$, and a new
{\it candidate} activation $\mathbf{\tilde{h}}_t$:
%
\vspace{-.2cm}
\begin{equation}
  \mathrm{GRU}(\mathbf{h}_{t-1}, \mathbf{x}_t) = (1 - \mathbf{z}_t)\odot \mathbf{h}_{t-1} + \mathbf{z}_t \odot \mathbf{\tilde{h}}_t
\vspace{-.1cm}
\end{equation}
%
where $\odot$ is elementwise multiplication, and the update gate
activation $\mathbf{z_{t}}$ determines the amount of new information
mixed in the current state:
%
\vspace{-.1cm}
\begin{equation}
\label{eq:gru-update}
   \mathbf{z}_t = \sigma_s(\mathbf{W}_z \mathbf{x}_t + \mathbf{U}_z \mathbf{h}_{t-1})
\vspace{-.1cm}
\end{equation}
%
The candidate activation is computed as:
%
\vspace{-.2cm}
\begin{equation}
\label{eq:gru-cand}
   \mathbf{\tilde{h}}_t = \sigma(\mathbf{W} \mathbf{x}_t + \mathbf{U}(\mathbf{r}_t \odot \mathbf{h}_{t-1}))
\vspace{-.1cm}
\end{equation}
%
The reset gate $\mathbf{r_{t}}$ determines how much of the current
input $\mathbf{x_{t}}$ is mixed in the previous state
$\mathbf{h}_{t-1}$ to form the candidate activation:
%
\vspace{-.2cm}
\begin{equation}
\label{eq:gru-reset}
   \mathbf{r}_t = \sigma_s(\mathbf{W}_r \mathbf{x}_t + \mathbf{U}_r \mathbf{h}_{t-1})
\vspace{-.1cm}
\end{equation}
where $\mathbf{W}$, $\mathbf{U}$, $\mathbf{W}_z$, $\mathbf{U}_z$, $\mathbf{W}_r$ and $\mathbf{U}_r$ are learnable parameters.



\subsection{Imaginet}
\label{sec:imaginet}



{\sc Imaginet} introduced in \namecite{chrupala2015learning} is a
multi-modal GRU network architecture that learns visually grounded
meaning representations from textual and visual input.  It acquires 
linguistic knowledge through language comprehension, by receiving a
description of a scene and trying to visualise it through predicting a visual
representation for the textual description, while concurrently predicting 
the next word in the sequence. 

\begin{figure}
\begin{center}
\includegraphics[scale=0.2]{imaginet.pdf} 
\caption{Structure of {\sc Imaginet}, adapted from \protect\cite{chrupala2015learning}.}
\label{fig:imaginet}
\end{center}
\end{figure}

Figure~\ref{fig:imaginet} shows the structure of {\sc Imaginet}. As can
be seen from the figure, the model consists of two GRU pathways, 
{\sc Textual} and {\sc Visual}, with a shared word embedding matrix. 
The inputs to the model are pairs of image descriptions and their 
corresponding images. The {\sc Textual} pathway predicts the next 
word at each position in the sequence of words in each caption, whereas the 
{\sc Visual} pathway predicts a visual representation of the image that depicts the 
scene described by the caption after the final word is received.

Formally, each sentence is mapped to two sequences of hidden states, 
one by {\sc Visual} and the other by {\sc Textual}:
\vspace{-.2cm}
\begin{align}
  \mathbf{h}^{V}_t = \mathrm{GRU}^V(\mathbf{h}^{V}_{t-1},\mathbf{x}_t)\\
  \mathbf{h}^{T}_t = \mathrm{GRU}^T(\mathbf{h}^{T}_{t-1},\mathbf{x}_t)
\vspace{-.1cm}
\end{align}
%
At each time step {\sc Textual} predicts the next word in the sentence
$S$ from its current hidden state $\mathbf{h}^{T}_t$, while {\sc
  Visual} predicts the image-vector\footnote{Representing the full image, 
  extracted from the pre-trained Convolutional Neural Network of 
  \namecite{simonyan2014very}. \label{edit:dumdumeddy}}
$\hat{\mathbf{i}}$ from its last
hidden representation $\mathbf{h}^{V}_t$.
%
\vspace{-.2cm}
\begin{align}
   \hat{\mathbf{i}} &= \mathbf{V} \mathbf{h}^{V}_\tau \\
    p(S_{t+1}|S_{1:t}) &= \mathrm{softmax}(\mathbf{L}\mathbf{h}^T_t)
\vspace{-.1cm}
\end{align}
%
The loss function is a multi-task objective which penalizes error on
the visual and the textual targets simultaneously. The objective
combines cross-entropy loss $L^{T}$ for the word predictions
and cosine distance $L^V$ for the image
predictions\footnote{Note that the original formulation in
  \namecite{chrupala2015learning} uses mean squared error instead; as the performance of
{\sc Visual} is measured on image-retrieval which is based on cosine 
distances, we use cosine distance as the visual loss here.\label{ft:imaginet}}, weighting them with the parameter $\alpha$ (set to $0.1$).

\begin{align}
&L^{T}(\theta) = {-} \frac{1}{\tau}\sum_{t=1}^\tau \log p(S_t|S_{1:t}) \\
&L^V(\theta) =  1 - \frac{\hat{\mathbf{i}} \cdot \mathbf{i}}{\| \hat{\mathbf{i}} \| \| \mathbf{i} \|} \\
\label{eq:losscombo}
&L = \alpha L^T + (1-\alpha)L^{V}
\end{align}

\noindent
For more details about the {\sc Imaginet} model and its performance
see \namecite{chrupala2015learning}. Note that we introduce a small change in
the image representation: we observe that using standardized image
vectors, where each dimension is transformed by subtracting the mean
and dividing by standard deviation, improves performance.




\subsection{Unimodal language model}
The model {\sc LM} is a language model analogous to the {\sc Textual}
pathway of {\sc Imaginet} with the difference that its word embeddings
are not shared, and its loss function is the cross-entropy on word
prediction. Using this model we remove the visual objective as a
factor, as the model does not use the images corresponding to captions
in any way.

\subsection{Sum of word embeddings}
The model {\sc Sum} is a stripped-down version of the {\sc Visual}
pathway, which does not share word embeddings, only uses the cosine
loss function, and replaces the GRU network with a summation over word
embeddings. This removes the effect of word order from
consideration. We use this model as a baseline in the sections which
focus on language structure. 

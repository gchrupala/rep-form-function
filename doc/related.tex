\section{Related work}
\label{sec:related}

The direct predecessors of modern architectures were first proposed in the 
seminal paper of \namecite{elman1990finding}. He modifies the recurrent 
neural network architecture of \namecite{jordan1986attractor} by changing the output-to-memory 
feedback connections to hidden-to-memory recurrence, enabling Elman networks to 
represent arbitrary dynamic systems. \namecite{elman1991distributed} trains an RNN on a small synthetic
sentence  dataset and analyzes the activation patterns of the hidden
layer. His analysis shows that these distributed representations  
encode lexical categories, grammatical relations and hierarchical constituent 
structures. \namecite{giles1992extracting} train RNNs similar to Elman
networks on strings generated by small 
deterministic regular grammars with the objective to recognize grammatical and reject ungrammatical 
strings, and develop the \emph{dynamic state partitioning} technique to extract the learned 
grammar from the networks in the form of deterministic finite state
automatons.
% Dynamic state partitioning follows the general pattern employed in the
% large body of literature of RNN rule extraction: (i) using quantization
% to create a mapping from the continuous RNN state- space to a finite
% set of discrete spaces; (ii) passing the RNNs input and observing the
% state transitions; (iii) rule construction; and (iv) rule
% minimization. We refer the reader to \cite{jacobsson2005rule} for a
% comprehensive survey on RNN rule extraction.

More closely related is the recent work of \namecite{li2015visualizing}, who develop techniques 
for a deeper understanding of the activation patterns of RNNs, but focus on models with 
modern architectures trained on large scale data sets. More specifically, they train Long 
Short-Term Memory networks (LSTM) \cite{hochreiter1997long} 
for phrase-level sentiment analysis 
and present novel methods 
to explore the inner workings of RNNs. They measure the salience of tokens
in sentences by taking the first-order derivatives of the loss with respect to the
word embeddings and provide evidence 
that LSTMs can learn to attend to important tokens in sentences. Furthermore,
they plot the activation values of hidden units through time using heat maps 
and visualize local semantic compositionality in RNNs. 
In comparison, the present work goes beyond the importance of single words and focuses more on exploring structure 
learning in RNNs, as well as on developing methods for a comparative analysis between RNNs that are focused on different modalities (language versus vision).

Adding an explicit attention mechanism that allows the RNNs to focus on \label{edit:attention}
different parts of the input was recently introduced by \namecite{bahdanau2014neural}
in the context of extending the sequence-to-sequence 
RNN architecture for neural machine translation. 
At the decoding side this neural module assigns weights 
to the hidden states of the decoder, which allows the decoder to selectively pay varying degrees of
attention to different phrases in the source sentence at different decoding time-steps.
They also provide qualitative analysis by visualizing the attention
weigths and exploring the importance of the source encodings at various decoding steps.
Similarly \namecite{rocktaschel2016reasoning} use an attentive neural 
network architecture to perform natural language inference and visualize which parts
of the hypotheses and premises the model pays attention to when deciding on the 
entailment relationship. Conversely, the present work focuses on RNNs without an 
explicit attention mechanism.

\namecite{karpathy2015visualizing} also take up the challenge of
rendering RNN activation patterns understandable, but use character
level language models and rather than taking a linguistic point of
view, focus on error analysis and training dynamics of LSTMs and
GRUs.  They show that certain dimensions in the RNN hidden activation
vectors have specific and interpretable functions. Similarly,
\namecite{li2015convergent} use a Convolutional Neural Networks (CNN)
based on the architecture of \namecite{krizhevsky2012imagenet}, 
and train it on the ImageNet dataset using different random initializations. For each layer
in all networks they store the activation values produced on the
validation set of ILSVRC and align similar neurons of different
networks. They conclude that while some features are learned across
networks, some seem to depend on the initialization. Other works on
visualizing the role of individual hidden units in deep models for vision
synthesize images by optimizing random images through backpropagation
to maximize the activity of units
\cite{erhan2009visualizing,simonyan2013deep,yosinski2015understanding,nguyen2016multifaceted}
or to approximate the activation vectors of particular layers
\cite{mahendran2015visualizing,dosovitskiy2015inverting}.

While this paper was under review, a number of articles appeared which also 
investigate linguistic representations in LSTM architectures. In an 
approach similar to ours, \namecite{li2016understanding} study the contribution of 
individual input tokens as well as hidden units and word embedding dimensions 
by erasing them from the representation and analyzing how this affects the model.
They focus on text-only tasks and do not take other 
modalities such as visual input into account.
\namecite{adi2016fine} take an alternative approach by introducing prediction tasks 
to analyze information encoded in sentence embeddings about sentence length, 
sentence content and word order. 
Finally, \namecite{linzen2016assessing} examine 
the acquisition of long-distance dependencies through the study of number agreement 
in different variations of an LSTM model with different objectives (number prediction, 
grammaticality judgment, and language modeling). Their results show that such dependencies 
can be captured with very high accuracy when the model receives a strong supervision 
signal (that is, whether the subject is plural or singular), but simple language models still capture 
the majority of test cases. While they focus on an in-depth analysis of a single phenomenon, in our work
we are interested in methods which make it possible to uncover a broad variety of patterns of bebavior in RNNs.   

In general, there has been a growing interest within computer vision
in understanding deep models, with a number of papers dedicated to
visualizing learned CNN filters and pixel saliencies
\cite{simonyan2013deep,yosinski2015understanding,mahendran2015understanding}. These
techniques have also led to improvements in model performance
\cite{eigen2013understanding} and transferability of features
\cite{zhou2014object}. To date there has been much less work on such
issues within computational linguistics. We aim to fill this gap by
adapting existing methods as well as developing novel techniques to
explore the linguistic structure learned by recurrent networks.

\section{Experiments}
\label{sec:experiments}


In this section, we report a series of experiments in which
we explore the kinds of linguistic regularities the networks learn from
word-level input. 
In Section~\ref{sec:computeomission} we introduce \emph{omission score},
a metric to measure the contribution of each token to the prediction
of the networks, and in Section~\ref{sec:omitimaginet} we analyze how
omission scores are distributed over dependency relations and
part-of-speech categories. 
In Section~\ref{sec:beyondlexical} we investigate the extent to which
the importance of words for the different networks depend on the words themselves,
their sequential position, and their grammatical function in the sentences.
Finally, in Section~\ref{sec:contexts} we systematically compare the types of 
n-gram contexts that trigger individual dimensions in the hidden layers of the 
networks, and discuss their level of abstractness.

In all these experiments we report our findings based on the {\sc Imaginet}
model, and whenever appropriate compare it to our two other models {\sc LM} and {\sc Sum}.
For all the experiments, we trained the models on the training portion of the
MSCOCO image-caption dataset \cite{lin2014microsoft}, and analyzed the
representations of the sentences in the validation set corresponding
to 5000 randomly chosen images. The target image representations were
extracted from the pre-softmax layer of the 16-layer CNN of
\namecite{simonyan2014very}.


\input{categories}
\input{dimensions}
